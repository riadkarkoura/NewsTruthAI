import streamlit as st
import requests
import pandas as pd
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
from translate import Translator  # Ù…ÙƒØªØ¨Ø© Ø§Ù„ØªØ±Ø¬Ù…Ø©
import nltk
nltk.download('stopwords')

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ±Ø¬Ù…Ø© Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
translator = Translator(to_lang="en", from_lang="ar")

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

# Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
df = pd.read_csv("liar_sample.csv")
label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['label'])
df['clean_text'] = df['text'].apply(clean_text)

X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'], df['label_encoded'], test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_tfidf, y_train)

# Ø¯Ø§Ù„Ø© Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù…Ù† NewsAPI
def get_latest_news(api_key, query="news", language="en", page_size=5):
    url = ('https://newsapi.org/v2/everything?'
           f'q={query}&'
           f'language={language}&'
           f'pageSize={page_size}&'
           'sortBy=publishedAt&'
           f'apiKey={api_key}')
    response = requests.get(url)
    data = response.json()
    if data['status'] == 'ok':
        articles = data['articles']
        news_texts = [article['title'] + ". " + article.get('description', '') for article in articles]
        return news_texts
    else:
        st.error("Error fetching news: " + data.get('message', 'Unknown error'))
        return []

def classify_arabic_news(arabic_text):
    try:
        translated_text = translator.translate(arabic_text)
        cleaned = clean_text(translated_text)
        vect = vectorizer.transform([cleaned])
        pred_encoded = model.predict(vect)[0]
        pred_label = label_encoder.inverse_transform([pred_encoded])[0]
        return pred_label, translated_text
    except Exception as e:
        return f"Error: {e}", ""

# ---- Streamlit UI ----

st.title("ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©")

api_key = st.text_input("Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ NewsAPI Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ:", value="cb432eea97dd4cec984e6917dae798bf", type="password")

if api_key:
    query = st.text_input("Ø§ÙƒØªØ¨ ÙƒÙ„Ù…Ø© Ø§Ù„Ø¨Ø­Ø« Ù„Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø© (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©):", value="Syria OR vaccine OR politics")

    if st.button("Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙˆØªØµÙ†ÙŠÙÙ‡Ø§"):
        lang = "en"
        if any('\u0600' <= c <= '\u06FF' for c in query):  # ØªØ­Ù‚Ù‚ Ù„Ùˆ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ø±Ø¨ÙŠ
            lang = "ar"

        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø¨Ø­Ø« Ø¹Ø±Ø¨ÙŠØŒ Ù†Ø­ØªØ§Ø¬ Ù†ØªØ±Ø¬Ù… Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù„Ø§Ø­Ù‚Ù‹Ø§ Ø¨Ø¹Ø¯ Ø¬Ù„Ø¨Ù‡Ø§
        if lang == "en":
            latest_news = get_latest_news(api_key, query=query, language=lang, page_size=5)
        else:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙÙ‚Ø· (NewsAPI Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø±Ø³Ù…ÙŠ) ÙˆÙ†ÙØªØ±Ø¶ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ Ù…ÙƒØ§ÙØ¦ Ù‡Ù†Ø§
            latest_news = get_latest_news(api_key, query="Syria OR vaccine OR politics", language="en", page_size=5)

        st.subheader("Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙˆØªØµÙ†ÙŠÙÙ‡Ø§:")
        for i, news in enumerate(latest_news, 1):
            if lang == "ar":
                # Ø¥Ø°Ø§ Ø£Ø±Ø¯Ù†Ø§ ØªØµÙ†ÙŠÙ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: Ù†ØªØ±Ø¬Ù… Ø§Ù„Ø®Ø¨Ø± Ø£ÙˆÙ„Ø§ Ø«Ù… Ù†ØµÙ†Ù
                label, translated = classify_arabic_news(news)
            else:
                cleaned_news = clean_text(news)
                vect_news = vectorizer.transform([cleaned_news])
                pred_encoded_news = model.predict(vect_news)[0]
                label = label_encoder.inverse_transform([pred_encoded_news])[0]
                translated = ""

            st.markdown(f"**{i}. Ø§Ù„Ø®Ø¨Ø±:** {news}")
            if translated:
                st.markdown(f"**ØªØ±Ø¬Ù…Ø© (Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©):** {translated}")
            st.markdown(f"**ØªØµÙ†ÙŠÙ:** {label}")
            st.write("---")

    st.subheader("ØªØµÙ†ÙŠÙ Ø®Ø¨Ø± Ø¹Ø±Ø¨ÙŠ Ù…Ø¨Ø§Ø´Ø±:")
    user_arabic_input = st.text_area("Ø£Ø¯Ø®Ù„ Ù†Øµ Ø®Ø¨Ø± Ø¹Ø±Ø¨ÙŠ Ù‡Ù†Ø§:")

    if st.button("ØªØµÙ†ÙŠÙ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ"):
        if user_arabic_input.strip() == "":
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø£ÙˆÙ„Ø§Ù‹.")
        else:
            label, translated = classify_arabic_news(user_arabic_input)
            st.markdown(f"**Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØªØ±Ø¬Ù…:** {translated}")
            st.markdown(f"**ØªØµÙ†ÙŠÙ Ø§Ù„Ø®Ø¨Ø±:** {label}")
    import streamlit as st

# --- Ø¹Ø±Ø¶ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØµØ­ÙÙŠ ÙÙŠ Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ ---
st.sidebar.markdown("## ğŸ‘¤ Riad Karkoura")
st.sidebar.markdown("**Tech Journalist | Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø¥Ø¹Ù„Ø§Ù… ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±**")

st.sidebar.markdown("[ğŸ”— Ø­Ø³Ø§Ø¨ÙŠ Ø¹Ù„Ù‰ LinkedIn](https://www.linkedin.com/in/riad-karkoura-b9010b196)")

