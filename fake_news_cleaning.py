import streamlit as st
import requests
import pandas as pd
import re
import nltk
import csv
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from deep_translator import GoogleTranslator
from transformers import pipeline
from pyairtable import Table

nltk.download('stopwords')

translator = GoogleTranslator(source='auto', target='en')
classifier = pipeline("text-classification", model="microsoft/xtremedistil-l6-h384-uncased")

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

# Ø¥Ø¹Ø¯Ø§Ø¯ Airtable
AIRTABLE_API_TOKEN = "YOUR_AIRTABLE_API_TOKEN"
AIRTABLE_BASE_ID = "appuBRk3WvvG8usrz"
AIRTABLE_TABLE_NAME = "NewsFeedback"
table = Table(AIRTABLE_API_TOKEN, AIRTABLE_BASE_ID, AIRTABLE_TABLE_NAME)

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

def get_latest_news(api_key, query="news", language="en", page_size=5):
    url = (
        'https://newsapi.org/v2/everything?'
        f'q={query}&'
        f'language={language}&'
        f'pageSize={page_size}&'
        'sortBy=publishedAt&'
        f'apiKey={api_key}'
    )
    response = requests.get(url)
    data = response.json()
    if data['status'] == 'ok':
        articles = data['articles']
        return [article['title'] + ". " + article.get('description', '') for article in articles]
    else:
        st.error("Error fetching news: " + data.get('message', 'Unknown error'))
        return []

def classify_arabic_news(text):
    try:
        translated = translator.translate(text)
        result = classifier(translated)[0]
        label = result['label']
        score = result['score']
        final_label = "Ø®Ø¨Ø± Ø­Ù‚ÙŠÙ‚ÙŠ âœ…" if label.upper() == "REAL" or label.upper() == "LABEL_1" else "Ø®Ø¨Ø± ÙƒØ§Ø°Ø¨ âŒ"
        return final_label, translated, score, label
    except Exception as e:
        return f"Error: {e}", "", 0, ""

def save_feedback(news, translated, predicted_label, score, journalist, feedback):
    try:
        table.create({
            "Original News": news,
            "Translated": translated,
            "Model Prediction": predicted_label,
            "Confidence": f"{score:.2%}",
            "Journalist": journalist,
            "Journalist Feedback": feedback
        })
        return True
    except Exception as e:
        st.error(f"ÙØ´Ù„ Ø­ÙØ¸ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…: {e}")
        return False

st.set_page_config(page_title="NewsTruth AI", layout="wide")
st.title("ğŸ“° NewsTruth AI â€“ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©")

api_key = st.text_input("ğŸ”‘ Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ NewsAPI Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ:", value="", type="password")

if api_key:
    query = st.text_input("ğŸ” Ø§ÙƒØªØ¨ ÙƒÙ„Ù…Ø© Ø¨Ø­Ø« (Ù…Ø«Ø§Ù„: Ø³ÙˆØ±ÙŠØ§ØŒ Ø³ÙŠØ§Ø³Ø©ØŒ Ù„Ù‚Ø§Ø­):", value="Syria OR vaccine")

    if st.button("ğŸ“¡ Ø¬Ù„Ø¨ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"):
        lang = "ar" if any('\u0600' <= c <= '\u06FF' for c in query) else "en"
        news_items = get_latest_news(api_key, query=query, language=lang, page_size=5)

        st.subheader("ğŸ—ï¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØµÙ†ÙØ©:")
        for i, news in enumerate(news_items, 1):
            if lang == "ar":
                label, translated, score, raw_label = classify_arabic_news(news)
            else:
                result = classifier(news)[0]
                raw_label = result['label']
                label = "Ø®Ø¨Ø± Ø­Ù‚ÙŠÙ‚ÙŠ âœ…" if raw_label.upper() == "REAL" or raw_label.upper() == "LABEL_1" else "Ø®Ø¨Ø± ÙƒØ§Ø°Ø¨ âŒ"
                translated, score = "", result['score']

            st.markdown(f"**{i}. Ø§Ù„Ø®Ø¨Ø±:** {news}")
            if translated:
                st.markdown(f"*Ø§Ù„ØªØ±Ø¬Ù…Ø©:* {translated}")
            st.markdown(f"ğŸ” **Ø§Ù„Ù†ØªÙŠØ¬Ø©:** {label} (Ø§Ù„Ø«Ù‚Ø©: {score:.2%})")

            journalist = st.text_input(f"ğŸ‘¤ Ø§Ø³Ù… Ø§Ù„ØµØ­ÙÙŠ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®Ø¨Ø± {i}", key=f"name_{i}")
            feedback = st.radio(
                f"Ù‡Ù„ ØªÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø®Ø¨Ø± {i}ØŸ",
                ["Ø£ÙˆØ§ÙÙ‚ âœ…", "Ù„Ø§ Ø£ÙˆØ§ÙÙ‚ âŒ"],
                key=f"feedback_{i}"
            )

            if st.button(f"ğŸ’¾ Ø­ÙØ¸ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®Ø¨Ø± {i}", key=f"save_{i}"):
                saved = save_feedback(news, translated, raw_label, score, journalist, feedback)
                if saved:
                    st.success("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­!")
            st.write("---")

    st.subheader("âœï¸ ØªØµÙ†ÙŠÙ Ø®Ø¨Ø± Ø¹Ø±Ø¨ÙŠ ÙŠØ¯ÙˆÙŠ:")
    user_input = st.text_area("Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ù‡Ù†Ø§:")

    if st.button("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø±"):
        if user_input.strip():
            label, translated, score, raw_label = classify_arabic_news(user_input)
            st.markdown(f"**ğŸ”„ Ø§Ù„ØªØ±Ø¬Ù…Ø©:** {translated}")
            st.markdown(f"**ğŸ” Ø§Ù„ØªØµÙ†ÙŠÙ:** {label} (Ø§Ù„Ø«Ù‚Ø©: {score:.2%})")

            journalist = st.text_input("ğŸ‘¤ Ø§Ø³Ù… Ø§Ù„ØµØ­ÙÙŠ Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®Ø¨Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ")
            feedback = st.radio("Ù‡Ù„ ØªÙˆØ§ÙÙ‚ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŸ", ["Ø£ÙˆØ§ÙÙ‚ âœ…", "Ù„Ø§ Ø£ÙˆØ§ÙÙ‚ âŒ"])

            if st.button("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù„Ø®Ø¨Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ"):
                saved = save_feedback(user_input, translated, raw_label, score, journalist, feedback)
                if saved:
                    st.success("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø¨Ù†Ø¬Ø§Ø­!")
        else:
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹.")

st.sidebar.markdown("## ğŸ‘¤ Riad Karkoura")
st.sidebar.markdown("ØµØ­ÙÙŠ ØªÙ‚Ù†ÙŠ | Ù…Ø®ØªØµ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")
st.sidebar.markdown("[ğŸ”— LinkedIn](https://www.linkedin.com/in/riad-karkoura-b9010b196)")
