import streamlit as st
import requests
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from translate import Translator
from transformers import pipeline

nltk.download('stopwords')

translator = Translator(to_lang="en", from_lang="ar")
classifier = pipeline("text-classification", model="mrm8488/bert-tiny-finetuned-fake-news")

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    words = [word for word in words if word not in stop_words]
    words = [stemmer.stem(word) for word in words]
    return ' '.join(words)

def get_latest_news(api_key, query="news", language="en", page_size=5):
    url = (
        'https://newsapi.org/v2/everything?'
        f'q={query}&'
        f'language={language}&'
        f'pageSize={page_size}&'
        'sortBy=publishedAt&'
        f'apiKey={api_key}'
    )
    response = requests.get(url)
    data = response.json()
    if data['status'] == 'ok':
        articles = data['articles']
        return [article['title'] + ". " + article.get('description', '') for article in articles]
    else:
        st.error("Error fetching news: " + data.get('message', 'Unknown error'))
        return []

def classify_arabic_news(text):
    try:
        translated = translator.translate(text)
        result = classifier(translated)[0]
        label = result['label']
        score = result['score']
        final_label = "Ø®Ø¨Ø± Ø­Ù‚ÙŠÙ‚ÙŠ âœ…" if label.lower() == "real" else "Ø®Ø¨Ø± ÙƒØ§Ø°Ø¨ âŒ"
        return final_label, translated, score
    except Exception as e:
        return f"Error: {e}", "", 0

# Streamlit interface
st.set_page_config(page_title="NewsTruth AI", layout="wide")
st.title("ğŸ“° NewsTruth AI â€“ ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©")

api_key = st.text_input("ğŸ”‘ Ø£Ø¯Ø®Ù„ Ù…ÙØªØ§Ø­ NewsAPI Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ:", value="", type="password")

if api_key:
    query = st.text_input("ğŸ” Ø§ÙƒØªØ¨ ÙƒÙ„Ù…Ø© Ø¨Ø­Ø« (Ù…Ø«Ø§Ù„: Ø³ÙˆØ±ÙŠØ§ØŒ Ø³ÙŠØ§Ø³Ø©ØŒ Ù„Ù‚Ø§Ø­):", value="Syria OR vaccine")

    if st.button("ğŸ“¡ Ø¬Ù„Ø¨ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ø­Ø¯ÙŠØ«Ø©"):
        lang = "ar" if any('\u0600' <= c <= '\u06FF' for c in query) else "en"
        news_items = get_latest_news(api_key, query="Syria OR vaccine", language="en", page_size=5)

        st.subheader("ğŸ—ï¸ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„Ù…ØµÙ†ÙØ©:")
        for i, news in enumerate(news_items, 1):
            if lang == "ar":
                label, translated, score = classify_arabic_news(news)
            else:
                result = classifier(news)[0]
                label = "Ø®Ø¨Ø± Ø­Ù‚ÙŠÙ‚ÙŠ âœ…" if result['label'].lower() == "real" else "Ø®Ø¨Ø± ÙƒØ§Ø°Ø¨ âŒ"
                translated, score = "", result['score']

            st.markdown(f"**{i}. Ø§Ù„Ø®Ø¨Ø±:** {news}")
            if translated:
                st.markdown(f"*Ø§Ù„ØªØ±Ø¬Ù…Ø©:* {translated}")
            st.markdown(f"ğŸ” **Ø§Ù„Ù†ØªÙŠØ¬Ø©:** {label} (Ø§Ù„Ø«Ù‚Ø©: {score:.2f})")
            st.write("---")

    st.subheader("âœï¸ ØªØµÙ†ÙŠÙ Ø®Ø¨Ø± Ø¹Ø±Ø¨ÙŠ ÙŠØ¯ÙˆÙŠ:")
    user_input = st.text_area("Ø£Ø¯Ø®Ù„ Ù†Øµ Ø§Ù„Ø®Ø¨Ø± Ù‡Ù†Ø§:")

    if st.button("ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¨Ø±"):
        if user_input.strip():
            label, translated, score = classify_arabic_news(user_input)
            st.markdown(f"**ğŸ”„ Ø§Ù„ØªØ±Ø¬Ù…Ø©:** {translated}")
            st.markdown(f"**ğŸ” Ø§Ù„ØªØµÙ†ÙŠÙ:** {label} (Ø§Ù„Ø«Ù‚Ø©: {score:.2f})")
        else:
            st.warning("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ù†Øµ Ø£ÙˆÙ„Ø§Ù‹.")

# Sidebar
st.sidebar.markdown("## ğŸ‘¤ Riad Karkoura")
st.sidebar.markdown("ØµØ­ÙÙŠ ØªÙ‚Ù†ÙŠ | Ù…Ø®ØªØµ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ø®Ø¨Ø§Ø±")
st.sidebar.markdown("[ğŸ”— LinkedIn](https://www.linkedin.com/in/riad-karkoura-b9010b196)")
